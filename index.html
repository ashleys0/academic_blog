<!DOCTYPE HTML>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-86220446-2', 'auto');
  ga('send', 'pageview');

</script>

<title>Been Kim</title>
<meta name="description" content="Just another Open Designs template." />
<meta name="robots" content="noodp,noydir" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" id="child-theme-css" href="css/style.css" type="text/css" media="all" />
<link rel="stylesheet" id="responsive-main-css-css" href="css/responsive-main.min.css" type="text/css" media="all" />
<link rel="stylesheet" id="responsive-css-css" href="css/responsive.css" type="text/css" media="all" />
<link rel="stylesheet" id="tb_styles-css" href="css/tb-styles.min.css" type="text/css" media="all" />
    <link href='http://fonts.googleapis.com/css?family=Cabin+Sketch:400,700' rel='stylesheet'   type='text/css'/>

<script type="text/javascript" src="js/jquery.js"></script>

<script type="text/javascript">
  jQuery(window).scroll(function (event) {
	  	
		var top = jQuery('#popular-upcoming').offset().top - jQuery(document).scrollTop();
		var top2 = jQuery('#contacts').offset() - jQuery(document).scrollTop();
		// what the y position of the scroll is
		var y = jQuery(this).scrollTop();
		// whether that's below the form
		if (y >= top)  {
		// if so, add the active class to popular-upcoming and remove from content
		jQuery('.page-nav-popular-posts').addClass('active');
		jQuery('.page-nav-top-posts').removeClass('active');
		} else {
		// otherwise remove it
		jQuery('.page-nav-popular-posts').removeClass('active');
		jQuery('.page-nav-top-posts').addClass('active');
	   }
  });
  
  jQuery(document).ready(function (){
  jQuery('#popular-scroll').click(function (){
            //jQuery(this).animate(function(){
                jQuery('html, body').animate({
                    scrollTop: jQuery('#popular-upcoming').offset().top
                     }, 1000);
            //});
        });

		jQuery('#contact-scroll').click(function (){
            //jQuery(this).animate(function(){
                jQuery('html, body').animate({
                    scrollTop: jQuery('#contact').offset().top
                     }, 1000);
            //});
        });
		
		jQuery('#feature-scroll').click(function (){
            //jQuery(this).animate(function(){
                jQuery('html, body').animate({
                    scrollTop: jQuery('#inner').offset().top
                     }, 1000);
            //});
        });
		  });
	  </script>
</head>

<body class="home blog header-full-width full-width-content">
  <div id="header">
  <div class="site-header">
    <h1 class="site-header-logo-container">
    <a href="/"><span class="image-replace">Name</span>
    <img src="images/mylogo.png" width="65" height="35" id="bigg-logo" alt="" /></a>
    </h1>
      
            <ul id="page-nav" class="horizontal-list">
<li class="page-nav-top-posts active"><a href="javascript:void(0)" id="feature-scroll" class="page-anchor-link">Home</a></li>
<li class="page-nav-popular-posts"><a href="javascript:void(0)" id="popular-scroll" class="page-anchor-link">Publications</a></li>
<li class="page-nav-contact-posts"><a href="javascript:void(0)" id="contact-scroll" class="page-anchor-link">Contact</a></li>

</ul>

<div id="site-nav" class="horizontal-list"><div class="menu-main-menu-container"><ul id="menu-main-menu" class="menu">
</ul></div></div><!-- #site-nav -->


<div id="site-header-bigg-social">
<ul class="horizontal-list">
<li><a href="https://twitter.com/_beenkim" target="_blank" class="bigg-social-twitter bigg-social-icon image-replace">Twitter</a></li>
<li><a href="http://scholar.google.com/citations?hl=en&user=aGXkhcwAAAAJ" target="_blank" class="bigg-social-gplus bigg-social-icon image-replace">Google+</a></li>

</ul>
      
    </div>  
    </div>
  </div>
  <div id="wrap">
<div id="inner">
<div class="wrap">
<div id="content-sidebar-wrap">
				
				<div id="content" class="hfeed">
				<div class="post-5 post type-post status-publish format-standard hentry category-featured category-parent-category-i entry feature feature">

		<img width="200" height="200" src="images/Been_Kim.jpg" class="alignleft post-image" alt="1" />
    <h3 class=cabin-sketch>
        <br/>
    <img src="images/mylogo.png" width="200" id="bigg-logo" alt="" />
    <br/>
    <p class=cabin-sketch>
	    Staff research scientist at Google Brain </p></h3>
      <br/>
      <br/>

    </h4>
		
				<div class="entry-content">
      <br/>
      <br/>
      <br/>
      <br/>
      <br/>
 	<p>
I am interested in designing high-performance machine learning methods that make sense <b>to humans</b>.
      <a href="https://www.quantamagazine.org/been-kim-is-building-a-translator-for-artificial-intelligence-20190110/"> Quanta magazine </a> described well why I am doing what I am doing. Thank you John Pavlus for writing this piece!
<!--Here are two reasons why <b>interpretable machine learning </b> matters. First, in order to enable ML to make real-world impact on important problems with serious consequences, such as medical diagnosis, policy making and disaster response, we need a system that can make sense to and be trusted by human experts. Only then will ML be trusted and widely used for these applications to make a difference in the world. The second reason is to prevent unfairness and social inequality. If ML, a very powerful tool, is not interpretable and can only be understood by a small number of highly specialized people or those who can afford them, it will deepen social inequality. By making ML models interpretable we can ensure that anyone can leverage these powerful tools.-->
<a href="https://www.facebook.com/WiMLWorkshop/photos/a.2187465001269129/1431021626913474/?type=3">Here</a> is another short writeup about why I care.
     
      <br/>
      <br/>
My focus is <b> building interpretability method for already-trained models </b>or <b>building inherently interpretable models </b>. In particular, I believe
the language of explanations should include <b>higher-level, human-friendly concepts</b> so that it can make sense to <b> everyone </b>.
      <br/>
      <br/>
 <!--     My work was also featured in an episode in <a href="https://www.thetalkingmachines.com/episodes/explainability-and-inexplicable"> Talking Machine </a> (which I have not had courage to listen myself talking). 
      Many thanks for Katherine Gorman!
      I built interpretable latent variable models (featured at <a href="http://www.thetalkingmachines.com/blog/2016/1/15/real-human-actions-and-women-in-machine-learning"> Talking Machines</a>, and 
      <a href="http://newsoffice.mit.edu/2014/pattern-recognition-systems-convey-learning-1205" >MIT news</a>)
      and creating structured Bayesian models of human decisions.
      I have applied these ideas to data from various domains: computer programming education, autism spectrum discorder data, recipes, disease data, 15 years of crime data from the city of Cambridge, human dialogue data from the AMI meeting corpus, and text-based chat data during disaster response. 
      I graduated with a PhD from <a href="http://csail.mit.edu/"> CSAIL</a>, <a href="http://mit.edu/"> MIT</a>.
(worked with 
      Prof. <a href="http://people.csail.mit.edu/julie_a_shah/Welcome.html" > Julie Shah </a> 
      and Prof. <a href="http://web.mit.edu/rudin/www/Mypage.html"> Cynthia Rudin</a>).-->
      <br/>
	  <br/>

    I gave a couple of tutorials on interpretability:
      <br/>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Deep Learning Summer school at University of Toronto, Vector institute in 2018 (<a href="slides/DLSS2018Vector_Been.pdf">slides</a>, <a href="https://vectorinstitute.ai/faq-items/2018-deep-learning-summer-school">video</a>)
      <br/>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CVPR 2018 (<a href="https://interpretablevision.github.io/index_cvpr2018.html">slides and videos</a>)
      <br/>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://2017.icml.cc/Conferences/2017/Tutorials">Tutorial on Interpretable machine learning at ICML 2017</a> (<a href="papers/BeenK_FinaleDV_ICML2017_tutorial.pdf">slides</a>,  <a href="https://icml.cc/Conferences/2017/Schedule?showEvent=900" >video</a>).
      <br/>
 	<p>

Other stuff I help with:
      <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Workshop Chair at ICLR 2018
      <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Area chair <a href="https://nips.cc/Conferences/2017/Committees">NIPS 2017, NeurIPS 2018, 2019, 2020</a>, ICML 2019, 2020, ICLR 2020, AISTATS 2020
      <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Senior program committee at AISTATS 2019
      <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Steering committee and area chair at <a href="https://fatconference.org/"> FAT* conference</a> 
      <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Program committee at ICML 2017/2018, AAAI 2017, IJCAI 2016 (and many other conference before that...)
      <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Executive board member of <a href="http://wimlworkshop.org/board/">  Women in Machine Learning</a>.
<!--My work on interpretable machine learing was feature at <a href="http://www.thetalkingmachines.com/blog/2016/1/15/real-human-actions-and-women-in-machine-learning"> Talk machines by Hanna Wallach</a> on January 14, 2016.-->
<br>      
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Co-organizer <a href="https://sites.google.com/corp/view/whi2018">3rd ICML 2018 Worshop on Human Interpretability in Machine Learning (WHI)</a>, <a href="https://sites.google.com/site/2016whi/">1st ICML 2016 Worshop on Human Interpretability in Machine Learning (WHI)</a>,
 <a href="https://sites.google.com/view/whi2017/home">2nd ICML 2017 Worshop on Human Interpretability in Machine Learning (WHI).</a>
and <a href="https://sites.google.com/site/nips2016interpretml">NIPS 2016 Worshop on Interpretable Machine Learning for Complex Systems</a>.       <br/>
      </p>
      </p>
     <br>
      </p>
      <br/>
      <a class="bigg-read-more" href="http://scholar.google.com/citations?hl=en&user=aGXkhcwAAAAJ">Google Scholar</a>
      <br/>

    </div><!-- end .entry-content -->
	</div><!-- end .postclass -->

<br>
<br>


     <hr color='#D6D6D6'>

      <h2> Preprints </h2>
   <!------------ start one posting ------------>
    <h5 class="entry-title"><a href="https://arxiv.org/abs/1903.01069" >Do Neural Networks Show Gestalt Phenomena? An Exploration of the Law of Closure
      </a>
    </h5> 
<b>
  TL;DR: It does. And it might be related to how NNs can generalize. 
</b>
<div class="entry-content">
<p>
Been Kim, Emily Reif, Martin Wattenberg, Samy Bengio
    [<a href="https://arxiv.org/abs/1903.01069">arxiv</a>]
    [<a href="https://www.technologyreview.com/s/613114/trained-neural-nets-perform-much-like-humans-on-classic-psychological-tests/">MIT Technology Review</a>]
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>
   
  
     <hr color='#D6D6D6'>

      <h2> Publications </h2>

		<div id="popular-upcoming" >
      </div>
    <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1910.07969" title=""><img width="350" height="213" src="images/completeConcept.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title"><a href="https://arxiv.org/abs/1910.07969" >
        On Completeness-aware Concept-Based Explanations in Deep Neural Networks
      </a>
    </h2> 
<b>
  TL;DR: Let's find set of concepts that are "sufficient" to explain predictions.
</b>
<div class="entry-content">
<p>
Chih-Kuan Yeh, Been Kim, Sercan O. Arik, Chun-Liang Li, Tomas Pfister, Pradeep Ravikumar
    <br />
    [<a href="https://arxiv.org/abs/1910.07969">Neurips 20</a>]
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>

     <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/2011.05429" title=""><img width="350" height="213" src="images/debugging2.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title"><a href="https://arxiv.org/abs/2011.05429" >
Debugging Tests for Model Explanations
      </a>
    </h2> 
<b>
  TL;DR: Sanity check2.
</b>
<div class="entry-content">
<p>
Julius Adebayo, Michael Muelly, Ilaria Liccardi, Been Kim
    <br />
    [<a href="https://arxiv.org/abs/2011.05429">Neurips 20</a>]
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>

    <br />
      <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/2007.04612" title=""><img width="350" height="200" src="images/cbm.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title"><a href="https://arxiv.org/abs/2007.04612" >
Concept Bottleneck Models
      </a>
    </h2> 
<b>
  TL;DR: Build a model where concepts are built-in so that you can control influential concepts.
</b>
<div class="entry-content">
<p>
Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, Percy Liang
    <br />
    [<a href="https://arxiv.org/abs/2007.04612">ICML 20</a>]
    [<a href="https://ai.googleblog.com/2021/01/google-research-looking-back-at-2020.html">Featured at Google Research review 2020</a>]
   
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>
 
    <br />
    <br />
    <br />
    <br />
    <br />
    <br />

   <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1907.07165" title=""><img width="350" height="213" src="images/cace.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1907.07165" >
        Explaining Classifiers with Causal Concept Effect (CaCE)
      </a>
    </h2> 
<b>
  TL;DR: Make TCAV causal.
</b>
<div class="entry-content">
<p>
Yash Goyal, Amir Feder, Uri Shalit, Been Kim
    [<a href="https://arxiv.org/abs/1907.07165">arxiv</a>]
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>
 
    <br />
    <br />
    <br />
    <br />

  
   <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1902.03129" title=""><img width="350" height="213" src="images/ace.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1902.03129" >
        Towards Automatic Concept-based Explanations
      </a>
    </h3> 
<b>
  TL;DR: Automatically discover high-level concepts that explain a model's prediction.
</b>
<div class="entry-content">
<p>
Amirata Ghorbani, James Wexler, James Zou, Been Kim
    <br />
    [<a href="https://arxiv.org/abs/1902.03129">Neurips 19</a>]
    [<a href="https://github.com/amiratag/ACE">code</a>]
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>
    <br />
    <br />
    <br />
    <br />
<!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1907.09701" title=""><img width="350" height="213" src="images/bim.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1907.09701" >
        BIM: Towards Quantitative Evaluation ofInterpretability Methods with Ground Truth
      </a>
    </h2> 
<b>
  TL;DR:  Ever thought "well, interpretability methods are hard to evaluate."? No more! We open source dataset, models and metrics that you can quantitatively evaluate your interpretability methods. We compare many widely used methods and publish their rankings.
</b>
<div class="entry-content">
<p>
Sharry Yang, Been Kim
    [<a href="https://arxiv.org/abs/1907.09701">arxiv</a>]
    [<a href="https://github.com/google-research-datasets/bim">code</a>]
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>
    <br />
    <br />
    <br />
    <br />
   <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1902.02715" title=""><img width="350" height="213" src="images/bert.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1906.02715" >
        Visualizing and Measuring the Geometry of BERT
      </a>
    </h3> 
<b>
  TL;DR: Studying geometry of BERT to gain insights behind their impressive performance.
</b>
<div class="entry-content">
<p>
Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viégas, Martin Wattenberg
    <br />
    [<a href="https://arxiv.org/abs/1906.02715">Neurips 19</a>]
    [<a href="https://pair-code.github.io/interpretability/bert-tree/">blog post</a>]
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>

    <br />
    <br />
    <br />
    <br />

 <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1806.10758" title=""><img width="350" height="213" src="images/roar.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1806.10758" >
        Evaluating Feature Importance Estimates
      </a>
    </h3> 
<b>
  TL;DR: One idea to evaluate attribution methods.
</b>
<div class="entry-content">
<p>
Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim
    <br />
    [<a href="https://arxiv.org/abs/1806.10758">Neurips 19</a>]
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>
    <br />
    <br />
    <br />
    
   <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1902.00006" title=""><img width="350" height="213" src="images/HumanEvalIke.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1902.00006" >
Human Evaluation of Models Built for Interpretability
      </a>
    </h3> 
<b>
  TL;DR: What are the factors of explanation that matter for better interpretability and in what setting? A large-scale study to answer this question.
</b>
<div class="entry-content">
<p>
Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Samuel Gershman and Finale Doshi-Velez
    <br />
    [<a href="https://arxiv.org/abs/1902.00006">HCOMP 19</a>]
    <b>(best paper honorable mention)</b><br />
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>
    <br />
    <br />
    <br />
 <br />
   
  <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1902.02960" title=""><img width="350" height="213" src="images/smily.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1902.02960" >
Human-Centered Tools for Coping with Imperfect Algorithms during Medical Decision-Making
      </a>
    </h2> 
<b>
  TL;DR: A tool to help doctors to navigate medical images using medically-relevant similarties. This work uses a part of <a href="https://arxiv.org/abs/1711.11279"> TCAV </a>idea to sort images with concepts.
</b>
<div class="entry-content">
<p>
Carrie J. Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S. Corrado, Martin C. Stumpe, Michael Terry
    <br />
    <i></i>CHI 2019 <b>(best paper honorable mention)</b><br />
    [<a href="https://arxiv.org/abs/1902.02960">pdf</a>]
    [<a href="bibs/smilyCHI19.bib">bibtex</a>]
     <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>

<!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1810.10118" title=""><img width="350" height="213" src="images/fisher.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1810.10118" >
        Interpreting Black Box Predictions using Fisher Kernels
      </a>
    </h2> 
<b>
  TL;DR: Answering "which training examples are most responsible for a given set of predictions?" Follow up of <a href="papers/KIM2016NIPS_MMD.pdf"> MMD-critic </a> [NeurIPS 16]. The difference is that now we pick examples informed by how the classifier sees them!
</b>
<div class="entry-content">
<p>
Rajiv Khanna, Been Kim, Joydeep Ghosh, Oluwasanmi Koyejo
    <br />
    <i></i>AISTATS 2019<br />
    [<a href="https://arxiv.org/abs/1810.10118">pdf</a>]
    [<a href="bibs/fisher19.bib">bibtex</a>]
     <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>


  <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1805.11783" title=""><img width="350" height="213" src="images/TrustScore.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1805.11783" >
        To Trust Or Not To Trust A Classifier
      </a>
    </h2> 
<b>
TL;DR: A very simple method that tells you whether to trust your prediction or not, that happens to also have nice theoretical properties! 
</b>
<div class="entry-content">
<p>
Heinrich Jiang, Been Kim, Melody Guan, Maya Gupta
    <br />
    <i></i>NIPS 2018 (poster)<br />
    [<a href="https://arxiv.org/abs/1805.11783">pdf</a>]
    [<a href="https://github.com/google/TrustScore">code</a>]
    [<a href="bibs/TrustScoreNIPS2018.bib">bibtex</a>]
     <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>

  <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1805.11571" title=""><img width="350" height="213" src="images/HumanInLoop.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1805.11571" >
        Human-in-the-Loop Interpretability Prior
      </a>
    </h2> 
<b>
TL;DR: Ask humans which models are more interpretable DURING the model training to learn more interpretable model for the end-task.
</b>
<div class="entry-content">
<p>
Isaac Lage, Andrew Slavin Ross, Been Kim, Samuel J. Gershman, Finale Doshi-Velez
    <br />
    <i></i>NIPS 2018 (spotlight)<br />
    [<a href="https://arxiv.org/abs/1805.11571">pdf</a>]
    [<a href="bibs/HumanInLoopNIPS2018.bib">bibtex</a>]
     <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>


  <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1810.03292" title=""><img width="350" height="213" src="images/SanityNIPS2018.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1810.03292" >
Sanity Checks for Saliency Maps
      </a>
    </h2> 
<b>
TL;DR: Saliency maps are a type of post-training interpretability method to explain 'evidence' of predictions. But it turns out that they have little to do with the model's prediction! Some saliency maps are visually indistinguishable before and after we randomize the weights of the network (i.e., producing garbage predictions)
</b>
<div class="entry-content">
<p>
Julius Adebayo, Justin Gilmer, Ian Goodfellow, Moritz Hardt, Been Kim
    <br />
    <i></i>NIPS 2018 (spotlight)<br />
    [<a href="https://arxiv.org/abs/1810.03292">pdf</a>]
    [<a href="bibs/SanityChecksNIPS2018.bib">bibtex</a>]
     <br/>
     <br/>
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>



  <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1711.11279" title=""><img width="350" height="213" src="images/TCAV_doctor.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1711.11279" >
Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)
      </a>
    </h2> 
<b>
TL;DR: We can learn human-concepts in any layer of already-trained neural networks. Then we can do hypothesis testing with them to get quantitative explanations.
</b>
<div class="entry-content">
<p>
Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, Rory Sayres 
    <br />
    <i></i>ICML 2018 <br />
    [<a href="https://arxiv.org/abs/1711.11279">pdf</a>]
    [<a href="https://github.com/tensorflow/tcav">code</a>]
    [<a href="bibs/TCAVKim17.bib">bibtex</a>]
    [<a href="slides/TCAV_ICML_pdf.pdf">slides</a>]
     <br/>
     <br/><img width="350" height="213" src="images/sundar.png" class="alignleft post-image" alt="2" />
     <a href="https://en.wikipedia.org/wiki/Sundar_Pichai"> Sundar Pichai </a> (CEO of Google)'s presenting TCAV as a tool to build AI for everyone at his keynote speech at Google I/O 2019
    [<a href="https://youtu.be/lyRPyRKHO8M?t=2156">video</a>]
     <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>

  <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1711.00867" title=""><img width="350" height="213" src="images/cat.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1711.00867" >
The (Un)reliability of saliency methods
      </a>
    </h2> 
<b>
TL;DR: Existing saliency methods could be unreliable. We should be careful using them.
</b>
<div class="entry-content">
<p>
Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schütt, Sven Dähne, Dumitru Erhan, Been Kim
    <br />
    <i></i>NIPS workshop 2017 on Explaining and Visualizing Deep Learning<br />
    [<a href="https://arxiv.org/abs/1711.00867">pdf</a>]
    [<a href="bibs/UnreliableKindermans17.bib">bibtex</a>]
     <br/>
     <br/>
     <br/>
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>


  <!------------ start one posting ------------>
		<a href="https://pair-code.github.io/saliency/" title=""><img width="350" height="213" src="images/smoothgrad.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://pair-code.github.io/saliency/" >
 SmoothGrad: removing noise by adding noise
      </a>
    </h2> 
<div class="entry-content">
<p>
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, Martin Wattenberg
    <br />
    <i></i>ICML workshop on Visualization for deep learning 2017 <br />
    [<a href="https://arxiv.org/abs/1706.03825">pdf</a>]
    [<a href="https://pair-code.github.io/saliency/">code</a>]
    [<a href="bibs/smoothgradSmilkov17.bib">bibtex</a>]
    <br/>
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>



  <!------------ start one posting ------------>
		<a href="http://nanchen.csie.org/publications/qsanglyzer_vast2017.pdf" title="Hello world!"><img width="350" height="213" src="images/qsangle.png" class="alignleft post-image" alt="2" /></a>		

    <h2 class="entry-title">
      <a href="http://nanchen.csie.org/publications/qsanglyzer_vast2017.pdf" >
QSAnglyzer: Visual Analytics for Prismatic Analysis of Question Answering System Evaluations
      </a>
    </h2> 
		<div class="entry-content">
			<p>
    Nan-chen Chen and Been Kim<br />
    <i></i>VAST 2017<br />
    [<a href="http://nanchen.csie.org/publications/qsanglyzer_vast2017.pdf">pdf</a>]
    [<a href="bibs/Chen2017">bibtex</a>]
    <br/>
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>



  <!------------ start one posting ------------>
		<a href="https://arxiv.org/abs/1702.08608" title="Hello world!"><img width="350" height="213" src="images/interpretabilityPaper.png" class="alignleft post-image" alt="2" /></a>		

    <h2 class="entry-title">
      <a href="https://arxiv.org/abs/1702.08608" >
Towards A Rigorous Science of Interpretable Machine Learning
      </a>
    </h2> 
		<div class="entry-content">
			<p>
    Finale Doshi-Velez and Been Kim<br />
    <i></i>[to appear] Springer Series on Challenges in Machine Learning: "Explainable and Interpretable Models in Computer Vision and Machine Learning", arxiv in 2017<br />
    [<a href="https://arxiv.org/abs/1702.08608">pdf</a>]
    [<a href="bibs/DoshiKIM2017">bibtex</a>]
    <br/>
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>


  <!------------ start one posting ------------>
		<a href="papers/KIM2016NIPS_MMD.pdf" title="Hello world!"><img width="350" height="213" src="images/nips2016_criticism.png" class="alignleft post-image" alt="2" /></a>		

    <h2 class="entry-title">
      <a href="papers/KIM2016NIPS_MMD.pdf" >
	Examples are not Enough, Learn to Criticize! Criticism for Interpretability
      </a>
    </h2> 
		<div class="entry-content">
			<p>
    Been Kim, Rajiv Khanna and Sanmi Koyejo<br />
    <i></i>Neural Information Processing Systems 2016 <br />
    [<a href="papers/KIM2016NIPS_MMD.pdf">pdf</a>]
    [<a href="slides/Kim2016MMD_final_short.pdf">NIPS oral presentation talk slides</a>]
[<a href="https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Examples-are-not-enough-learn-to-criticize-Criticism-for-Interpretability">talk video</a>]
    [<a href="bibs/KIM2016NIPS_MMD_bib">bibtex</a>]
    [<a href="https://github.com/BeenKim/MMD-critic">code</a>]
    <br/>
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>

  <!------------ start one posting ------------>
		<a href="" title="Hello world!"><img width="350" height="213" src="images/nips2016_diff.png" class="alignleft post-image" alt="2" /></a>		

    <h2 class="entry-title">
      <a href="" >
Diff-clustering: Interpretable embedding example-based clustering
      </a>
    </h2> 
		<div class="entry-content">
			<p>
    Been Kim, Peter Turney and Peter Clark<br />
    <i></i>under review<br />
			[TBD]
    <br/>
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>


  <!------------ start one posting ------------>
		<a href="papers/BKim2015NIPS.pdf" title="Hello world!"><img width="350" height="213" src="images/nips2015.png" class="alignleft post-image" alt="2" /></a>		

    <h2 class="entry-title">
      <a href="papers/BKim2015NIPS.pdf" >
		Mind the Gap: A Generative Approach to Interpretable Feature Selection and Extraction
      </a>
    </h2> 
		<div class="entry-content">
			<p>
    Been Kim, Finale Doshi-Velez and Julie Shah<br />
    <i></i>Neural Information Processing Systems 2015<br />
    [<a href="papers/BKim2015NIPS.pdf">pdf</a>]
    [<a href="papers/BKim2015NIPS_supp.pdf">variational inference in gory detail</a>]
    [<a href="bibs/BKimDoshiVelezShahNIPS2015">bibtex</a>]
    <br/>
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>


  <!------------ start one posting ------------>
		<a href="papers/.pdf" title="Hello world!"><img width="350" height="213" src="images/ibcm_thumb.png" class="alignleft post-image" alt="2" /></a>		

    <h2 class="entry-title">
      <a href="papers/.pdf" >
        iBCM: Interactive Bayesian Case Model Empowering Humans via Intuitive Interaction
      </a>
    </h2> 
		<div class="entry-content">
			<p>
    Been Kim, Elena Glassman, Brittney Johnson and Julie Shah<br />
    coming soon (see my thesis for details).
    <!--[<a href="papers/.pdf">pdf</a>]-->
    <!--[<a href="bibs/">bibtex</a>]-->
    <br/>
    [<a href="https://www.youtube.com/watch?v=8PwHigCDdW8&feature=youtu.be">video</a>]
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>


      <!------------ start one posting ------------>
		<a href="papers/KimRudinShahNIPS2014.pdf" title="Hello world!"><img width="350" height="213" src="images/nips14_thumb.png" class="alignleft post-image" alt="2" /></a>		

    <h2 class="entry-title">
      <a href="papers/KimRudinShahNIPS2014.pdf" >
        Bayesian Case Model: <br/> A Generative Approach for Case-Based Reasoning and Prototype Classification</a>
    </h2> 
		<div class="entry-content">
			<p>
    Been Kim, Cynthia Rudin and Julie Shah<br />
    <i></i>Neural Information Processing Systems 2014<br />
    [<a href="papers/KimRudinShahNIPS2014.pdf">pdf</a>]
    [<a href="papers/BeenKimNIPS2014Poster.pdf">poster</a>]
    [<a href="bibs/BKimRudinShahNIPS14">bibtex</a>]
    <br/>
    <p style="color:darkblue">
This work was featured on 
<a href="http://newsoffice.mit.edu/2014/pattern-recognition-systems-convey-learning-1205">MIT news</a> and <a href="http://web.mit.edu/site/date/2014/12/">MIT front page spotlight</a>.
</p>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>

      <!------------ start one posting ------------>
   <a href="papers/BeenPRSAAAI15.pdf"><img width="350" height="213" src="images/aaai15_thumb.png" class="alignleft post-image" alt="2" /></a>		

    <h2 class="entry-title">
        <a href="papers/BeenPRSAAAI15.pdf">
          Scalable and interpretable data representation for <br/> high-dimensional complex data</a>
    </h2> 
		<div class="entry-content">
			<p>
    Been Kim, Kayur Patel, Afshin Rostamizadeh and Julie Shah<br />
    <i></i>AAAI Conference on Artificial Intelligence 2015 <br / >
    [<a href="papers/BeenPRSAAAI15.pdf">pdf</a>]
    [<a href="bibs/BKimAAAI15">bibtex</a>]
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->
      <!------------ end one posting ------------>

      <!------------ start one posting ------------>
		<a href="papers/KimShah14JAIR.pdf" ><img width="350" height="168" src="images/jair14_thumb.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="papers/KimShah14JAIR.pdf" > 
	A Bayesian Generative Modeling with Logic-Based Prior</a></h2>
				<div class="entry-content">
          <p>Been Kim, Caleb Chacha and Julie Shah<br />
Journal of Artificial Intelligence Research 2014<br /> 
[<a href="papers/KimShah14JAIR.pdf">pdf</a>]
[<a href="bibs/BKimCalebShahJAIR14">bibtex</a>]
<br/>
<br/>
<br/>
    </div><!-- end .entry-content -->
     <!------------ end one posting ------------>

      <!------------ start one posting ------------>
		<a href="papers/KimRudin2014.pdf" title="A Simple Text Post"><img width="350" height="168" src="images/dami_thumb.png" class="alignleft post-image" alt="5" /></a>		
    <h2 class="entry-title"><a href="papers/KimRudin2014.pdf" title="A Simple Text Post" rel="bookmark"> 
	Learning about Meetings</a></h2>
				<div class="entry-content">
<p>Been Kim and Cynthia Rudin <br />
<i></i>Data Mining and Knowledge Discovery Journal
2014 <br />
<br />
[<a href="http://arxiv.org/abs/1306.1927">arxiv</a>]
[<a href="papers/KimRudin2014.pdf">pdf</a>]
[<a href="bibs/BKimRudinDAMI14">bibtex</a>]	
<br />
    <p style="color:darkblue">
This work was featured in <a href="http://online.wsj.com/news/articles/SB10001424127887324021104578553900295324678">Wall Street Journal</a>.
</p>
<br />
<br />
			</p>
    </div><!-- end .entry-content -->
     <!------------ end one posting ------------>
      <!------------ start one posting ------------>
<a href="papers/BKimCS2013.pdf" title="Hello world!"><img width="350" height="168" src="images/aaai13_thumb.png" class="alignleft post-image" alt="6" /></a>		
    <h2 class="entry-title"><a href="papers/BKimCS2013.pdf" title="Hello world!" rel="bookmark"> 
        Inferring Robot Task Plans from Human Team Meetings: <br/>A Generative Modeling Approach with Logic-Based Prior
</a></h2>
				<div class="entry-content">
          <p>
Been Kim, Caleb Chacha and Julie Shah<br/>
AAAI Conference on Artificial Intelligence 2013 <br / >
[<a href="papers/BKimCS2013.pdf">pdf</a>]
[<a href="bibs/BKimAAAI13">bibtex</a>]	
[<a href="http://tiny.cc/uxhcrw">video</a>]	


<br/>
<br/>
    <p style="color:darkblue">
This work was featured in:
<br/>
"Introduction to AI" course at Harvard (COMPSCI180: Computer science 182) by Barbara J. Grosz.
<br/>
[<a href="http://isites.harvard.edu/icb/icb.do?keyword=k97104">Course website</a>]
<br/>
"Human in the loop planning and decision support" tutorial at AAAI15 by Kartik Talamadupula and Subbarao Kambhampati.
<br/>
[<a href="http://rakaposhi.eas.asu.edu/HIL-Final-Combined.pdf">slides From the tutorial</a>]
</p>
</td></tr>
			</p>
    </div><!-- end .entry-content -->
     <!------------ end one posting ------------>
      <!------------ start one posting ------------>
		<a href="papers/BKim10ICRA.pdf" title="A Post With Everything In It"><img width="350" height="168" src="images/icra10_thumb.png" class="alignleft post-image" alt="7" /></a>		
    <h2 class="entry-title"><a href="papers/BKim10ICRA.pdf" title="A Post With Everything In It" rel="bookmark"> 
		Multiple Relative Pose Graphs for Robust Cooperative Mapping</a></h2>

				<div class="entry-content">
	<p>
Been Kim, Michael Kaess, Luke Fletcher, John Leonard, Abraham Bachrach, Nicholas Roy, and Seth Teller <br />
International Conference on Robotics and Automation 2010 <br />
[<a href="papers/BKim10ICRA.pdf">pdf</a>]
[<a href="bibs/BKimICRA10">bibtex</a>]
[<a href="http://youtu.be/_d6p9VhDsF8">video</a>]

		</p>
    </div><!-- end .entry-content -->
	
     <!------------ end one posting ------------>

      <!------------ start one posting ------------>
		<a href="papers/ShahKimNikolaidisAAAITechReport.pdf" title="A Post With Everything In It"><img width="350" height="168" src="images/symp_thumb.png" class="alignleft post-image" alt="7" /></a>		
    <h2 class="entry-title"><a href="papers/ShahKimNikolaidisAAAITechReport.pdf" title="A Post With Everything In It" rel="bookmark"> 
Human-inspired Techniques for Human-Machine Team Planning<br />
</a></h2>
				<div class="entry-content">
	<p>
Julie Shah, Been Kim and Stefanos Nikolaidis<br/>
AAAI Technical Report - Human Control of Bioinspired Swarms 2013 <br / >
[<a href="papers/ShahKimNikolaidisAAAITechReport.pdf">pdf</a>]
[<a href="bibs/ShahKimNikolaidisAAAISymp">bibtex</a>]
		</p>
    <br/>
    <br/>
    <br/>
    </div><!-- end .entry-content -->



     <!------------ end one posting ------------>
      <!------------ start one posting ------------>
     <!------------ end one posting ------------>
      		
     <hr color='#D6D6D6'>
      <h2> Thesis</h2>

		<a href="papers/BKimPhDThesis.pdf" title="Hello world!"><img width="350" height="213" src="images/thesis_thumb.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="papers/BKimPhDThesis.pdf" >
      Interactive and Interpretable Machine Learning Models for Human Machine Collaboration
      </a>
    </h2> 
		<div class="entry-content">
			<p>
    Been Kim<br />
    <i></i>PhD Thesis 2015<br />
    [<a href="papers/BKimPhDThesis.pdf">pdf</a>]
    [<a href="bibs/BKimPhDThesisBib">bibtex</a>]
    [<a href="papers/BKimPhDThesis_slides">slides</a>]
</p>
    </div><!-- end .entry-content -->
    <br >
    <br >
    <br >
    <br >
		

     <hr color='#D6D6D6'>
		<div id="selected-press" >
      <h2> Selected Press (out of date) </h2>
      <br/>
<br>
      
 
      <a href="https://www.thetalkingmachines.com/episodes/explainability-and-inexplicable"> 
<img width="350" height="213" src="images/talkingmachines.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="https://www.thetalkingmachines.com/episodes/explainability-and-inexplicable" >
     Talking machines
      </a>
    </h2> 
		<div class="entry-content">
			<p>
      Talking machines 2018
    [<a href=
"https://www.thetalkingmachines.com/episodes/explainability-and-inexplicable" 
>article</a>]
    </p>

    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    </div><!-- end .entry-content -->
      

      
      <a href="http://www.thetalkingmachines.com/blog/2016/1/15/real-human-actions-and-women-in-machine-learning" >
<img width="350" height="213" src="images/talkingmachines.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="http://www.thetalkingmachines.com/blog/2016/1/15/real-human-actions-and-women-in-machine-learning" >
     Talking machines
      </a>
    </h2> 
		<div class="entry-content">
			<p>
      Talking machines 2016
    [<a href=
"http://www.thetalkingmachines.com/blog/2016/1/15/real-human-actions-and-women-in-machine-learning" 
>article</a>]
    </p>

    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    </div><!-- end .entry-content -->
      

		<a href="http://newsoffice.mit.edu/2014/pattern-recognition-systems-convey-learning-1205
" ><img width="250" height="213" src="images/mitnews_thumb_2.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="http://newsoffice.mit.edu/2014/pattern-recognition-systems-convey-learning-1205
" >
      Computers that teach by example
      </a>
    </h2> 
		<div class="entry-content">
			<p>
      MIT news 2014
    [<a href="http://newsoffice.mit.edu/2014/pattern-recognition-systems-convey-learning-1205
">article</a>]
    </p>
    </div><!-- end .entry-content -->
    <br>
      

   	<a href="http://online.wsj.com/news/articles/SB10001424127887324021104578553900295324678
" ><img width="350" height="213" src="images/wsj_thumb.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="http://online.wsj.com/news/articles/SB10001424127887324021104578553900295324678
" >
      At Work: Just Say `Yeah’
      </a>
    </h2> 
		<div class="entry-content">
			<p>
      Wall Street Journal 2013
    [<a href="http://online.wsj.com/news/articles/SB10001424127887324021104578553900295324678
">article</a>]
    </p>
    </div><!-- end .entry-content -->
    
      
	<a href="http://www.forbes.com/sites/dandiamond/2013/06/23/how-to-win-co-workers-and-influence-meetings/" ><img width="200" height="213" src="images/forbes_thumb.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="http://www.forbes.com/sites/dandiamond/2013/06/23/how-to-win-co-workers-and-influence-meetings/" >
      How To Win Over Co-Workers And Influence Meetings: Use These 3 Words
      </a>
    </h2> 
		<div class="entry-content">
			<p>
      Forbes 2013
    [<a href="http://www.forbes.com/sites/dandiamond/2013/06/23/how-to-win-co-workers-and-influence-meetings/">article</a>]
    </p>
    </div><!-- end .entry-content -->
   
      

	<a href="http://abcnews.go.com/blogs/business/2013/06/researchers-discover-the-key-to-persuasion/" ><img width="250" height="213" src="images/abcnews_thumb.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="http://abcnews.go.com/blogs/business/2013/06/researchers-discover-the-key-to-persuasion/" >
      Researchers Discover the Key to Persuasion 
      </a>
    </h2> 
		<div class="entry-content">
			<p>ABC news 2013
    [<a href="http://abcnews.go.com/blogs/business/2013/06/researchers-discover-the-key-to-persuasion/">article</a>]
    </p>
    </div><!-- end .entry-content -->

 	<a href="http://news.yahoo.com/5-words-results-business-meetings-102239872.html" ><img width="250" height="213" src="images/yahoo_thumb.png" class="alignleft post-image" alt="2" /></a>		
    <h2 class="entry-title">
      <a href="http://news.yahoo.com/5-words-results-business-meetings-102239872.html" >
      5 Important Words to Say in Every Business Meeting
      </a>
    </h2> 
		<div class="entry-content">
			<p>Yahoo news 2013
    [<a href="http://news.yahoo.com/5-words-results-business-meetings-102239872.html">article</a>]
    </p>
    </div><!-- end .entry-content -->
     



     <hr color='#D6D6D6'>
		<div id="contact" >
      <h2> Contact </h2>
      <br/>
      <br/>
      <br/>
    <img src="images/email.jpg" width="200" id="bigg-logo" alt="" /></a>
	</div><!-- end .wrap -->
</div><!-- end #inner --> 

  <!--
<div id="bigg-footer">

<div class="wrap">
        <div class="twocol">
            <div id="text-2" class="widget widget_text"><div class="widget-wrap"><h4 class="widgettitle">Company</h4>			
			<div class="textwidget"><ul class="plain-list">
<li><a href="#">About</a></li>
<li><a href="#">Jobs</a></li>
<li><a href="#">Contact</a></li>
<li><a href="#">Terms</a></li>
<li><a href="#">Privacy</a></li>
</ul>
</div>
		</div></div>
        </div>
        <div class="twocol">
            <div id="text-3" class="widget widget_text"><div class="widget-wrap"><h4 class="widgettitle">Community</h4>			<div class="textwidget"><ul class="plain-list">
<li><a href="#">Blog</a></li>
<li><a href="#">Twitter</a></li>
<li><a href="#">Facebook</a></li>
<li><a href="#">Help</a></li>
</ul>
</div>
		</div></div>
        </div>
        <div class="fourcol">
            <div id="text-4" class="widget widget_text"><div class="widget-wrap"><h4 class="widgettitle">Subscribe to the newsletter</h4>			<div class="textwidget"><p>Top stories from Bigg delivered to your inbox</p>

<div>
<input type="text" placeholder="Enter your email address" name="email" class="form-field" id="newsletter-email-input"> <input type="button" value="Submit" class="button" id="newsletter-email-submit-btn">
</div>

<p class="legalese">
Opt-out anytime with one click and we'll never share your information.
</p></div>
		</div></div>
        </div>
        <div class="fourcol last">
            <div id="text-5" class="widget widget_text"><div class="widget-wrap"><h4 class="widgettitle">Free download</h4>			<div class="textwidget"><p>Bigg is a completely free website template that you can download and start tinkering with right away.</p>

<a class="button" href="http://www.opendesigns.org/design/bigg/">Download now!</a></div>
		</div></div>
        </div>
		            </div>
</div>
 
</div><!-- end #wrap -->

</div>
</body>
</html>
    <div class="footer-copyright clear">
© <span id="footer-copyright-year">2013</span> Bigg | Built by <a href="http://www.opendesigns.org/">OD</a>
</div>

